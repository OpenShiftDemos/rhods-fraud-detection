{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff01632",
   "metadata": {},
   "source": [
    "## Creating Pipelines\n",
    "\n",
    "In notebooks [2](02-feature-engineering.ipynb) and  [3](03-model-logistic-regression.ipynb) we developed and trained a feature engineering technique and a logistic regression model. In this notebook we will combine them into a pipeline. \n",
    "\n",
    "Machine learning pipelines allow you to precisely specify a set of transformations which start with raw data and result in a model. They make it possible to re-train the same model repeatedly, using different parameter values, and to reapply these same transformations to raw data in production, resulting in predictions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8fb7cb",
   "metadata": {},
   "source": [
    "We load in our data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8764a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(\"fraud-cleaned-sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c8e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train, test = model_selection.train_test_split(df, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01187836",
   "metadata": {},
   "source": [
    "Now we load the pipeline steps we created in earier notebooks. These are `feat_pipeline.pkl` and `lr.pkl`, corresponding to the feature engineering stages and the logisitc regression model, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a44f55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle as cp\n",
    "feature_pipeline = cp.load(open('feat_pipeline.pkl', 'rb'))\n",
    "model = cp.load(open('lr.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6bf68",
   "metadata": {},
   "source": [
    "Now we can combine these stages together in a pipeline and fit it to training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd7afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('features', feature_pipeline),\n",
    "    ('model', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd600031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 Pipeline(steps=[('feature_extraction',\n",
       "                                  ColumnTransformer(transformers=[('interarrival_scaler',\n",
       "                                                                   Pipeline(steps=[('median_imputer',\n",
       "                                                                                    SimpleImputer(strategy='median')),\n",
       "                                                                                   ('interarrival_scaler',\n",
       "                                                                                    RobustScaler())]),\n",
       "                                                                   ['interarrival']),\n",
       "                                                                  ('amount_scaler',\n",
       "                                                                   RobustScaler(),\n",
       "                                                                   ['amount']),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder(categories=[['online',\n",
       "                                                                                              'contactless',\n",
       "                                                                                              'chip_and_pin',\n",
       "                                                                                              'manual',\n",
       "                                                                                              'swipe']],\n",
       "                                                                                 handle_unknown='ignore'),\n",
       "                                                                   ['trans_type']),\n",
       "                                                                  ('m_hashing',\n",
       "                                                                   Pipeline(steps=[('dictify',\n",
       "                                                                                    FunctionTransformer(accept_sparse=True,\n",
       "                                                                                                        func=<function amap at 0x7fdf586e1ee0>)),\n",
       "                                                                                   ('hasher',\n",
       "                                                                                    FeatureHasher(n_features=128))]),\n",
       "                                                                   'merchant_id')]))])),\n",
       "                ('model', LogisticRegression(max_iter=500))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954011ef",
   "metadata": {},
   "source": [
    "Here you can see all the transformations and parameters used in the pipeline. \n",
    "\n",
    "We can refit the whole pipeline to training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f69751fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 Pipeline(steps=[('feature_extraction',\n",
       "                                  ColumnTransformer(transformers=[('interarrival_scaler',\n",
       "                                                                   Pipeline(steps=[('median_imputer',\n",
       "                                                                                    SimpleImputer(strategy='median')),\n",
       "                                                                                   ('interarrival_scaler',\n",
       "                                                                                    RobustScaler())]),\n",
       "                                                                   ['interarrival']),\n",
       "                                                                  ('amount_scaler',\n",
       "                                                                   RobustScaler(),\n",
       "                                                                   ['amount']),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder(categories=[['online',\n",
       "                                                                                              'contactless',\n",
       "                                                                                              'chip_and_pin',\n",
       "                                                                                              'manual',\n",
       "                                                                                              'swipe']],\n",
       "                                                                                 handle_unknown='ignore'),\n",
       "                                                                   ['trans_type']),\n",
       "                                                                  ('m_hashing',\n",
       "                                                                   Pipeline(steps=[('dictify',\n",
       "                                                                                    FunctionTransformer(accept_sparse=True,\n",
       "                                                                                                        func=<function amap at 0x7fdf586e1ee0>)),\n",
       "                                                                                   ('hasher',\n",
       "                                                                                    FeatureHasher(n_features=128))]),\n",
       "                                                                   'merchant_id')]))])),\n",
       "                ('model', LogisticRegression(max_iter=500))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(train, y = train[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7688f",
   "metadata": {},
   "source": [
    "We can use this pipeline to make predictions - let's predict for our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df78f137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['legitimate', 'legitimate', 'legitimate', ..., 'legitimate',\n",
       "       'legitimate', 'legitimate'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a1a7a",
   "metadata": {},
   "source": [
    "Let's now save this pipeline as one pickled object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6ee541",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.dump(pipeline, open(\"pipeline.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c33ae2",
   "metadata": {},
   "source": [
    "With the pipeline saved, we can now start to think about developing a model service that we can interact with, rather than just crunching data through the jupyter notebook. Head over to the 'app' folder inside this file directory to access the next notebooks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
